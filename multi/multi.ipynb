{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation_score_torch_faster(y_true, y_pred):\n",
    "    \"\"\"Compute the correlation between each rows of the y_true and y_pred tensors.\n",
    "    Compatible with backpropagation.\n",
    "    \"\"\"\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1)[:,None]\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:,None]\n",
    "    cov_tp = torch.sum(y_true_centered*y_pred_centered, dim=1)/(y_true.shape[1]-1)\n",
    "    var_t = torch.sum(y_true_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    var_p = torch.sum(y_pred_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    return cov_tp/torch.sqrt(var_t*var_p)\n",
    "\n",
    "def correl_loss(pred, tgt):\n",
    "    \"\"\"Loss for directly optimizing the correlation.\n",
    "    \"\"\"\n",
    "    return -torch.mean(partial_correlation_score_torch_faster(tgt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fcs = nn.Sequential(\n",
    "                    nn.Linear(228942, 512),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(128, 23418),\n",
    "                )\n",
    "    def forward(self, input_data):\n",
    "        feature = self.fcs(input_data)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation_score_torch_faster(y_true, y_pred):\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1)[:,None]\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:,None]\n",
    "    cov_tp = torch.sum(y_true_centered*y_pred_centered, dim=1)/(y_true.shape[1]-1)\n",
    "    var_t = torch.sum(y_true_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    var_p = torch.sum(y_pred_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    return cov_tp/torch.sqrt(var_t*var_p)\n",
    "\n",
    "def correl_loss(pred, tgt):\n",
    "    return -torch.mean(partial_correlation_score_torch_faster(tgt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def train_one_epoch(loader_size, batch_size, net, criterion, optimizer, scheduler, use_cuda):\n",
    "    loss_lst = []\n",
    "    for i in range(0, 50000//loader_size):\n",
    "        train_input = pd.read_hdf('./train_multi_inputs.h5', start= loader_size * i, stop=loader_size * i + loader_size)\n",
    "        train_target = pd.read_hdf('./train_multi_targets.h5', start= loader_size * i, stop=loader_size * i + loader_size)\n",
    "        x_train = train_input.to_numpy()\n",
    "        y_train = train_target.to_numpy()\n",
    "        x_train = torch.Tensor(x_train).to(device)\n",
    "        y_train = torch.Tensor(y_train).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train, y_train)\n",
    "        dataloader= DataLoader(train_dataset, shuffle = True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "        loss_sum = 0\n",
    "        for batch_idx, (inputs, target) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, target = Variable(inputs), Variable(target)\n",
    "            \n",
    "            pred_target = net(inputs)\n",
    "            \n",
    "            loss = criterion(target, pred_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                loss_sum += loss.detach().cpu().numpy()\n",
    "        \n",
    "        loss_lst.append(loss_sum)\n",
    "        print(\"#%d load, loss sum: %.4f\" % (i, loss_sum))\n",
    "        if(i % 3 == 0): scheduler.step()\n",
    "\n",
    "    return loss_lst\n",
    "\n",
    "def val_one_epoch(loader_size, batch_size, net, use_cuda):\n",
    "    loss_lst = []\n",
    "    partial_correlation_scores = []\n",
    "    for i in range(75000//loader_size, 100000//loader_size):\n",
    "        train_input = pd.read_hdf('./train_multi_inputs.h5', start= loader_size * i, stop=loader_size * i + loader_size)\n",
    "        train_target = pd.read_hdf('./train_multi_targets.h5', start= loader_size * i, stop=loader_size * i + loader_size)\n",
    "        x_train = train_input.to_numpy()\n",
    "        y_train = train_target.to_numpy()\n",
    "\n",
    "        x_train = torch.Tensor(x_train).to(device)\n",
    "        y_train = torch.Tensor(y_train).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train, y_train)\n",
    "        dataloader = DataLoader(train_dataset, shuffle = True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "        for batch_idx, (inputs, target) in enumerate(dataloader):\n",
    "            inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_target = net(inputs)\n",
    "            \n",
    "            score = partial_correlation_score_torch_faster(target, pred_target)\n",
    "            partial_correlation_scores.append(score)\n",
    "\n",
    "    partial_correlation_scores = torch.cat(partial_correlation_scores)\n",
    "    score = torch.sum(partial_correlation_scores).cpu().item()/len(partial_correlation_scores)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "net = Net()\n",
    "use_cuda = False\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    cudnn.benchmark = True\n",
    "    net = net.cuda()\n",
    "    # criterion = criterion.cuda()\n",
    "    use_cuda = True\n",
    "\n",
    "criterion = correl_loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 load, loss sum: -1.8684\n",
      "#1 load, loss sum: -4.3051\n",
      "#2 load, loss sum: -5.0066\n",
      "#3 load, loss sum: -5.1047\n",
      "#4 load, loss sum: -5.0796\n",
      "#5 load, loss sum: -5.1575\n",
      "#6 load, loss sum: -5.1784\n",
      "#7 load, loss sum: -5.2303\n",
      "#8 load, loss sum: -5.3288\n",
      "#9 load, loss sum: -5.3516\n",
      "#10 load, loss sum: -5.3715\n",
      "#11 load, loss sum: -5.3689\n",
      "#12 load, loss sum: -5.3662\n",
      "#13 load, loss sum: -4.6091\n",
      "#14 load, loss sum: -4.6498\n",
      "#15 load, loss sum: -4.6947\n",
      "#16 load, loss sum: -4.8574\n",
      "#17 load, loss sum: -5.1407\n",
      "#18 load, loss sum: -5.2734\n",
      "#19 load, loss sum: -5.3299\n",
      "#20 load, loss sum: -5.3161\n",
      "#21 load, loss sum: -5.1964\n",
      "#22 load, loss sum: -5.2335\n",
      "#23 load, loss sum: -5.2452\n",
      "Train: ep: 0, loss: -4.9693\n",
      "Val: ep: 0, score: 0.6527\n",
      "#0 load, loss sum: -5.3317\n",
      "#1 load, loss sum: -5.3788\n",
      "#2 load, loss sum: -5.3975\n",
      "#3 load, loss sum: -5.2900\n",
      "#4 load, loss sum: -5.1605\n",
      "#5 load, loss sum: -5.2005\n",
      "#6 load, loss sum: -5.2059\n",
      "#7 load, loss sum: -5.2450\n",
      "#8 load, loss sum: -5.3290\n",
      "#9 load, loss sum: -5.3495\n",
      "#10 load, loss sum: -5.3714\n",
      "#11 load, loss sum: -5.3695\n",
      "#12 load, loss sum: -5.3674\n",
      "#13 load, loss sum: -4.5970\n",
      "#14 load, loss sum: -4.6006\n",
      "#15 load, loss sum: -4.6598\n",
      "#16 load, loss sum: -4.9022\n",
      "#17 load, loss sum: -5.1290\n",
      "#18 load, loss sum: -5.2275\n",
      "#19 load, loss sum: -5.2958\n",
      "#20 load, loss sum: -5.2913\n",
      "#21 load, loss sum: -5.2031\n",
      "#22 load, loss sum: -5.2234\n",
      "#23 load, loss sum: -5.2341\n",
      "Train: ep: 1, loss: -5.1817\n",
      "Val: ep: 1, score: 0.6532\n",
      "#0 load, loss sum: -5.3637\n",
      "#1 load, loss sum: -5.3728\n",
      "#2 load, loss sum: -5.3837\n",
      "#3 load, loss sum: -5.3015\n",
      "#4 load, loss sum: -5.1767\n",
      "#5 load, loss sum: -5.1969\n",
      "#6 load, loss sum: -5.2000\n",
      "#7 load, loss sum: -5.2392\n",
      "#8 load, loss sum: -5.3180\n",
      "#9 load, loss sum: -5.3362\n",
      "#10 load, loss sum: -5.3604\n",
      "#11 load, loss sum: -5.3618\n",
      "#12 load, loss sum: -5.3621\n",
      "#13 load, loss sum: -4.5872\n",
      "#14 load, loss sum: -4.5642\n",
      "#15 load, loss sum: -4.6123\n",
      "#16 load, loss sum: -4.9318\n",
      "#17 load, loss sum: -5.1804\n",
      "#18 load, loss sum: -5.2280\n",
      "#19 load, loss sum: -5.2761\n",
      "#20 load, loss sum: -5.2663\n",
      "#21 load, loss sum: -5.2185\n",
      "#22 load, loss sum: -5.2291\n",
      "#23 load, loss sum: -5.2294\n",
      "Train: ep: 2, loss: -5.1790\n",
      "Val: ep: 2, score: 0.6538\n",
      "#0 load, loss sum: -5.3761\n",
      "#1 load, loss sum: -5.3716\n",
      "#2 load, loss sum: -5.3701\n",
      "#3 load, loss sum: -5.3017\n",
      "#4 load, loss sum: -5.1967\n",
      "#5 load, loss sum: -5.2055\n",
      "#6 load, loss sum: -5.2007\n",
      "#7 load, loss sum: -5.2379\n",
      "#8 load, loss sum: -5.3149\n",
      "#9 load, loss sum: -5.3278\n",
      "#10 load, loss sum: -5.3489\n",
      "#11 load, loss sum: -5.3502\n",
      "#12 load, loss sum: -5.3517\n",
      "#13 load, loss sum: -4.5846\n",
      "#14 load, loss sum: -4.5476\n",
      "#15 load, loss sum: -4.5805\n",
      "#16 load, loss sum: -4.9445\n",
      "#17 load, loss sum: -5.2293\n",
      "#18 load, loss sum: -5.2546\n",
      "#19 load, loss sum: -5.2825\n",
      "#20 load, loss sum: -5.2604\n",
      "#21 load, loss sum: -5.2277\n",
      "#22 load, loss sum: -5.2364\n",
      "#23 load, loss sum: -5.2306\n",
      "Train: ep: 3, loss: -5.1805\n",
      "Val: ep: 3, score: 0.6545\n",
      "#0 load, loss sum: -5.3742\n",
      "#1 load, loss sum: -5.3666\n",
      "#2 load, loss sum: -5.3591\n",
      "#3 load, loss sum: -5.2977\n",
      "#4 load, loss sum: -5.2077\n",
      "#5 load, loss sum: -5.2135\n",
      "#6 load, loss sum: -5.2047\n",
      "#7 load, loss sum: -5.2416\n",
      "#8 load, loss sum: -5.3207\n",
      "#9 load, loss sum: -5.3292\n",
      "#10 load, loss sum: -5.3462\n",
      "#11 load, loss sum: -5.3447\n",
      "#12 load, loss sum: -5.3445\n",
      "#13 load, loss sum: -4.5889\n",
      "#14 load, loss sum: -4.5440\n",
      "#15 load, loss sum: -4.5673\n",
      "#16 load, loss sum: -4.9515\n",
      "#17 load, loss sum: -5.2561\n",
      "#18 load, loss sum: -5.2751\n",
      "#19 load, loss sum: -5.2949\n",
      "#20 load, loss sum: -5.2656\n",
      "#21 load, loss sum: -5.2318\n",
      "#22 load, loss sum: -5.2395\n",
      "#23 load, loss sum: -5.2314\n",
      "Train: ep: 4, loss: -5.1832\n",
      "Val: ep: 4, score: 0.6548\n",
      "#0 load, loss sum: -5.3752\n",
      "#1 load, loss sum: -5.3654\n",
      "#2 load, loss sum: -5.3547\n",
      "#3 load, loss sum: -5.2955\n",
      "#4 load, loss sum: -5.2125\n",
      "#5 load, loss sum: -5.2174\n",
      "#6 load, loss sum: -5.2071\n",
      "#7 load, loss sum: -5.2447\n",
      "#8 load, loss sum: -5.3264\n",
      "#9 load, loss sum: -5.3327\n",
      "#10 load, loss sum: -5.3474\n",
      "#11 load, loss sum: -5.3440\n",
      "#12 load, loss sum: -5.3422\n",
      "#13 load, loss sum: -4.5943\n",
      "#14 load, loss sum: -4.5446\n",
      "#15 load, loss sum: -4.5623\n",
      "#16 load, loss sum: -4.9555\n",
      "#17 load, loss sum: -5.2692\n",
      "#18 load, loss sum: -5.2843\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_lst = []\n",
    "for epoch in range (0, 10):\n",
    "    train_loss_lst = train_one_epoch(2048, 256, net, criterion, optimizer, scheduler, True)\n",
    "\n",
    "    epoch_loss = Average(train_loss_lst)\n",
    "    epoch_loss_lst.append(epoch_loss)\n",
    "    print(\"Train: ep: %d, loss: %.4f\" % (epoch, epoch_loss))\n",
    "\n",
    "    val_score = val_one_epoch(2048, 256, net, use_cuda)\n",
    "    print(\"Val: ep: %d, score: %.4f\" % (epoch, val_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./multi_MSE.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
